{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Debugging autoreload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pytorch_tabular.utils import load_covertype_dataset\n",
    "from rich.pretty import pprint\n",
    "import torch\n",
    "from glob import glob\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pytorch_tabular.utils import make_mixed_dataset, print_metrics\n",
    "from pytorch_tabular import available_models\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig, GatedAdditiveTreeEnsembleConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.tabular_model_tuner import TabularModelTuner\n",
    "from torchmetrics.functional.classification import (\n",
    "    multiclass_accuracy,\n",
    "    multiclass_f1_score,\n",
    "    multiclass_precision,\n",
    "    multiclass_recall,\n",
    "    multiclass_specificity,\n",
    "    multiclass_cohen_kappa,\n",
    "    multiclass_auroc\n",
    ")\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "import pandas as pd\n",
    "from pytorch_tabular import model_sweep\n",
    "from src.pt.model_sweep import model_sweep_custom\n",
    "import warnings\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.utils.hash import dict_hash\n",
    "from pytorch_tabular.utils import get_balanced_sampler\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "path_data = \"D:/YandexDisk/Work/bbd/immunology/002_central_vs_yakutia/classification\"\n",
    "path_configs = \"D:/Work/bbs/notebooks/immunology/002_central_vs_yakutia/pt_configs\"\n",
    "data = pd.read_excel(f\"{path_data}/data.xlsx\", index_col=0)\n",
    "feats = pd.read_excel(f\"{path_data}/feats.xlsx\", index_col=0).index.values.tolist()\n",
    "\n",
    "test_split_id = 0\n",
    "\n",
    "val_n_splits = 4\n",
    "val_random_state = 1337\n",
    "val_fold_id = 0\n",
    "\n",
    "for fold_id in range(val_n_splits):\n",
    "    data[f\"Fold_{fold_id}\"] = data[f\"Split_{test_split_id}\"]\n",
    "\n",
    "stratify_cat_parts = {\n",
    "    'Central': data.index[(data['Region'] == 'Central') & (data[f\"Split_{test_split_id}\"] == 'trn_val')].values,\n",
    "    'Yakutia': data.index[(data['Region'] == 'Yakutia') & (data[f\"Split_{test_split_id}\"] == 'trn_val')].values,\n",
    "}\n",
    "for part, ids in stratify_cat_parts.items():\n",
    "    print(f\"{part}: {len(ids)}\")\n",
    "    con = data.loc[ids, 'Age'].values\n",
    "    ptp = np.ptp(con)\n",
    "    num_bins = 5\n",
    "    bins = np.linspace(np.min(con) - 0.1 * ptp, np.max(con) + 0.1 * ptp, num_bins + 1)\n",
    "    binned = np.digitize(con, bins) - 1\n",
    "    unique, counts = np.unique(binned, return_counts=True)\n",
    "    occ = dict(zip(unique, counts))\n",
    "    k_fold = RepeatedStratifiedKFold(\n",
    "        n_splits=val_n_splits,\n",
    "        n_repeats=1,\n",
    "        random_state=val_random_state\n",
    "    )\n",
    "    splits = k_fold.split(X=ids, y=binned, groups=binned)\n",
    "    \n",
    "    for fold_id, (ids_trn, ids_val) in enumerate(splits):\n",
    "        data.loc[ids[ids_trn], f\"Fold_{fold_id}\"] = \"trn\"\n",
    "        data.loc[ids[ids_val], f\"Fold_{fold_id}\"] = \"val\"\n",
    "        \n",
    "test = data.loc[data[f\"Split_{test_split_id}\"] == \"tst\", feats + ['Region']]\n",
    "train_validation = data.loc[data[f\"Split_{test_split_id}\"] == \"trn_val\", feats + ['Region'] + [f\"Fold_{i}\" for i in range(val_n_splits)]]\n",
    "train_only = data.loc[data[f\"Fold_{val_fold_id}\"] == \"trn\", feats + ['Region']]\n",
    "validation_only = data.loc[data[f\"Fold_{val_fold_id}\"] == \"val\", feats + ['Region']]\n",
    "cv_indexes = [\n",
    "    (\n",
    "        np.where(train_validation.index.isin(train_validation.index[train_validation[f\"Fold_{i}\"] == 'trn']))[0],\n",
    "        np.where(train_validation.index.isin(train_validation.index[train_validation[f\"Fold_{i}\"] == 'val']))[0],\n",
    "    )\n",
    "    for i in range(val_n_splits)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare balanced sampler"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sampler_balanced = get_balanced_sampler(train_only['Region'].values.ravel())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models Search Spaces"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CategoryEmbeddingModel Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__layers\": [\"256-128-64\", \"512-256-128\", \"32-16\", \"32-32-16\", \"16-8\", \"32-16-8\", \"128-64\", \"128-128\", \"16-16\"],\n",
    "    \"model_config.head_config__dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [42, 1337, 666],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/CategoryEmbeddingModelConfig.yaml\", CategoryEmbeddingModelConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GANDALF Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__gflu_stages\": [5, 10, 15, 20, 25, 30, 35],\n",
    "    \"model_config__gflu_dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__gflu_feature_init_sparsity\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"model_config.head_config__dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337, 666],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/GANDALFConfig.yaml\", GANDALFConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TabNetModel Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__n_d\": [8, 16, 24, 32, 40, 48],\n",
    "    \"model_config__n_a\": [8, 16, 24, 32, 40, 48],\n",
    "    \"model_config__n_steps\": [3, 5, 7],\n",
    "    \"model_config__gamma\": [1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n",
    "    \"model_config__n_independent\": [1, 2, 3, 4, 5],\n",
    "    \"model_config__n_shared\": [1, 2, 3, 4, 5],\n",
    "    \"model_config__mask_type\": [\"sparsemax\", \"entmax\"],\n",
    "    \"model_config.head_config__dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337, 666],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/TabNetModelConfig.yaml\", TabNetModelConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DANet Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__n_layers\": [4, 8, 16, 20, 32],\n",
    "    \"model_config__abstlay_dim_1\": [8, 16, 32, 64],\n",
    "    \"model_config__k\": [3, 4, 5, 6, 7],\n",
    "    \"model_config__dropout_rate\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config.head_config__dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337, 666],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/DANetConfig.yaml\", DANetConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FTTransformer Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__num_heads\": [2, 4, 8, 16, 32],\n",
    "    \"model_config__num_attn_blocks\": [4, 6, 8, 10, 12],\n",
    "    \"model_config__attn_dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__add_norm_dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__ff_dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config.head_config__dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337, 666],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/FTTransformerConfig.yaml\", FTTransformerConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grid Search and Random Search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "\n",
    "strategy = 'random_search' # 'grid_search'\n",
    "seed = 1337\n",
    "n_random_trials = 250\n",
    "is_cross_validation = True\n",
    "\n",
    "if grid_size < n_random_trials and strategy == 'random_search':\n",
    "    strategy = 'grid_search'\n",
    "\n",
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "data_config['continuous_feature_transform'] = 'yeo-johnson'\n",
    "data_config['normalize_continuous_features'] = True\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "trainer_config['checkpoints'] = None\n",
    "trainer_config['load_best'] = False\n",
    "trainer_config['auto_lr_find'] = True\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "\n",
    "tuner = TabularModelTuner(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    suppress_lightning_logger=True,\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    if is_cross_validation:\n",
    "        result = tuner.tune(\n",
    "            train=train_validation,\n",
    "            validation=None,\n",
    "            search_space=search_space,\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            strategy=strategy,\n",
    "            n_trials=n_random_trials,\n",
    "            cv=cv_indexes,\n",
    "            return_best_model=True,\n",
    "            verbose=False,\n",
    "            progress_bar=False,\n",
    "            random_state=seed,\n",
    "            train_sampler=sampler_balanced\n",
    "        )\n",
    "    else: \n",
    "        result = tuner.tune(\n",
    "            train=train_only,\n",
    "            validation=validation_only,\n",
    "            search_space=search_space,\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            strategy=strategy,\n",
    "            n_trials=n_random_trials,\n",
    "            cv=None,\n",
    "            return_best_model=True,\n",
    "            verbose=False,\n",
    "            progress_bar=False,\n",
    "            random_state=seed,\n",
    "            train_sampler=sampler_balanced\n",
    "        )\n",
    "\n",
    "result.trials_df.to_excel(f\"{trainer_config['checkpoints_path']}/trials/{model_config['_model_name']}_{strategy}_{seed}_{optimizer_config['lr_scheduler']}.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Sweep Training"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate models' configs from trials files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_top_trials = 50\n",
    "\n",
    "target_models_types = [\n",
    "    'CategoryEmbeddingModel',\n",
    "    'GANDALF',\n",
    "    'TabNetModel',\n",
    "    'DANet',\n",
    "    # 'FTTransformer'\n",
    "]\n",
    "\n",
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "\n",
    "common_params = {\n",
    "    \"task\": \"classification\",\n",
    "}\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    activation='ReLU',\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=False,\n",
    "    initialization=\"kaiming\"\n",
    ").__dict__\n",
    "\n",
    "model_list = []\n",
    "for model_type in target_models_types:\n",
    "    trials_files = glob(f\"{trainer_config['checkpoints_path']}/trials/{model_type}*.xlsx\")\n",
    "    for trials_file in trials_files:\n",
    "        df_trials = pd.read_excel(trials_file, index_col=0)\n",
    "        df_trials.sort_values(['accuracy'], ascending=[True], inplace=True)\n",
    "        df_trials = df_trials.head(n_top_trials)\n",
    "        for _, row in df_trials.iterrows():\n",
    "            head_config_tmp = copy.deepcopy(head_config)\n",
    "            head_config_tmp['dropout'] = float(row['model_config.head_config__dropout'])\n",
    "            if model_type == 'CategoryEmbeddingModel':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", CategoryEmbeddingModelConfig)\n",
    "                model_config['layers'] = row['model_config__layers']\n",
    "                model_config['learning_rate'] = row['model_config__learning_rate']\n",
    "                model_config['seed'] = row['model_config__seed']\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(CategoryEmbeddingModelConfig(**model_config))\n",
    "            elif model_type == 'GANDALF':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", GANDALFConfig)\n",
    "                model_config['gflu_stages'] = int(row['model_config__gflu_stages'])\n",
    "                model_config['gflu_feature_init_sparsity'] = float(row['model_config__gflu_feature_init_sparsity'])\n",
    "                model_config['gflu_dropout'] = float(row['model_config__gflu_dropout'])\n",
    "                model_config['learning_rate'] = float(row['model_config__learning_rate'])\n",
    "                model_config['seed'] = int(row['model_config__seed'])\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(GANDALFConfig(**model_config))\n",
    "            elif model_type == 'TabNetModel':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", TabNetModelConfig)\n",
    "                model_config['n_steps'] = row['model_config__n_steps']\n",
    "                model_config['n_shared'] = row['model_config__n_shared']\n",
    "                model_config['n_independent'] = row['model_config__n_independent']\n",
    "                model_config['n_d'] = row['model_config__n_d']\n",
    "                model_config['n_a'] = row['model_config__n_a']\n",
    "                model_config['mask_type'] = row['model_config__mask_type']\n",
    "                model_config['gamma'] = row['model_config__gamma']\n",
    "                model_config['learning_rate'] = row['model_config__learning_rate']\n",
    "                model_config['seed'] = row['model_config__seed']\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(TabNetModelConfig(**model_config))\n",
    "            elif model_type == 'FTTransformer':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", FTTransformerConfig)\n",
    "                model_config['num_heads'] = int(row['model_config__num_heads'])\n",
    "                model_config['num_attn_blocks'] = int(row['model_config__num_attn_blocks'])\n",
    "                model_config['attn_dropout'] = float(row['model_config__attn_dropout'])\n",
    "                model_config['add_norm_dropout'] = float(row['model_config__add_norm_dropout'])\n",
    "                model_config['ff_dropout'] = float(row['model_config__ff_dropout'])\n",
    "                model_config['learning_rate'] = float(row['model_config__learning_rate'])\n",
    "                model_config['seed'] = int(row['model_config__seed'])\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(FTTransformerConfig(**model_config))\n",
    "            elif model_type == 'DANet':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", DANetConfig)\n",
    "                model_config['n_layers'] = int(row['model_config__n_layers'])\n",
    "                model_config['abstlay_dim_1'] = int(row['model_config__abstlay_dim_1'])\n",
    "                model_config['k'] = int(row['model_config__k'])\n",
    "                model_config['dropout_rate'] = float(row['model_config__dropout_rate'])\n",
    "                model_config['learning_rate'] = float(row['model_config__learning_rate'])\n",
    "                model_config['seed'] = int(row['model_config__seed'])\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(DANetConfig(**model_config))\n",
    "print(len(model_list))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Perform model sweep"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "\n",
    "for seed in [1337, 55763, 40279, 8751, 234461]:\n",
    "\n",
    "    trainer_config['seed'] = seed\n",
    "    trainer_config['checkpoints'] = 'valid_loss'\n",
    "    trainer_config['load_best'] = True\n",
    "    trainer_config['auto_lr_find'] = True\n",
    "    \n",
    "    data_config['continuous_feature_transform'] = 'yeo-johnson' #  'box-cox' 'yeo-johnson' 'quantile_normal'\n",
    "    data_config['normalize_continuous_features'] = True\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        sweep_df, best_model = model_sweep_custom(\n",
    "            task=\"classification\",\n",
    "            # train=train_validation,\n",
    "            train=train_only,\n",
    "            # validation=None,\n",
    "            validation=validation_only,\n",
    "            test=test,\n",
    "            data_config=data_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            trainer_config=trainer_config,\n",
    "            model_list=model_list,\n",
    "            common_model_args=common_params,\n",
    "            metrics=[\n",
    "                \"accuracy\",\n",
    "                \"f1_score\",\n",
    "                \"precision\",\n",
    "                \"recall\",\n",
    "                \"specificity\",\n",
    "                \"cohen_kappa\",\n",
    "                \"auroc\"\n",
    "            ],\n",
    "            metrics_params=[\n",
    "                {'task': 'multiclass', 'num_classes': 2, 'average': 'macro'},\n",
    "                {'task': 'multiclass', 'num_classes': 2, 'average': 'macro'},\n",
    "                {'task': 'multiclass', 'num_classes': 2, 'average': 'macro'},\n",
    "                {'task': 'multiclass', 'num_classes': 2, 'average': 'macro'},\n",
    "                {'task': 'multiclass', 'num_classes': 2, 'average': 'macro'},\n",
    "                {'task': 'multiclass', 'num_classes': 2},\n",
    "                {'task': 'multiclass', 'num_classes': 2, 'average': 'macro'},\n",
    "            ],\n",
    "            metrics_prob_input=[True, True, True, True, True, True, True],\n",
    "            rank_metric=(\"accuracy\", \"higher_is_better\"),\n",
    "            return_best_model=True,\n",
    "            seed=seed,\n",
    "            progress_bar=False,\n",
    "            verbose=False,\n",
    "            suppress_lightning_logger=True,\n",
    "            train_sampler=sampler_balanced\n",
    "        )\n",
    "    fn_suffix = f\"{seed}_{best_model.config['lr_scheduler']}_{best_model.config['continuous_feature_transform']}\"\n",
    "    sweep_df.style.background_gradient(\n",
    "        subset=[\n",
    "            \"train_loss\",\n",
    "            \"validation_loss\",\n",
    "            \"test_loss\",\n",
    "            \"time_taken\",\n",
    "            \"time_taken_per_epoch\"\n",
    "        ], cmap=\"RdYlGn_r\"\n",
    "    ).to_excel(f\"{trainer_config['checkpoints_path']}/sweep_{fn_suffix}.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save best models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "\n",
    "sweep_df = pd.read_excel(f\"{trainer_config['checkpoints_path']}/sweep_1337_CosineAnnealingWarmRestarts_yeo-johnson.xlsx\", index_col=0)\n",
    "\n",
    "model_id = 197\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=ast.literal_eval(sweep_df.at[model_id, 'params']),\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    verbose=True,\n",
    "    suppress_lightning_logger=False\n",
    ")\n",
    "datamodule = tabular_model.prepare_dataloader(\n",
    "    train=train_only,\n",
    "    validation=validation_only,\n",
    "    seed=seed,\n",
    ")\n",
    "model = tabular_model.prepare_model(\n",
    "    datamodule\n",
    ")\n",
    "tabular_model._prepare_for_training(\n",
    "    model,\n",
    "    datamodule\n",
    ")\n",
    "tabular_model.load_weights(sweep_df.at[model_id, 'checkpoint'])\n",
    "tabular_model.evaluate(test, verbose=False)\n",
    "tabular_model.save_model(f\"{tabular_model.config['checkpoints_path']}/candidates/{model_id}\")\n",
    "\n",
    "loaded_model = TabularModel.load_model(f\"{tabular_model.config['checkpoints_path']}/candidates/{model_id}\")\n",
    "\n",
    "df = data.loc[:, ['Age', 'SImAge', 'Sex', 'Region']]\n",
    "df.loc[train_only.index, 'Group'] = 'Train'\n",
    "df.loc[validation_only.index, 'Group'] = 'Validation'\n",
    "df.loc[test.index, 'Group'] = 'Test'\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df,\n",
    "        loaded_model.predict(data),\n",
    "        loaded_model.predict(data, ret_logits=True).loc[:, ['logits_0', 'logits_1']]\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "df.rename(columns={'prediction': 'Prediction', 'logits_0': 'Central_logits', 'logits_1': 'Yakutia_logits'},\n",
    "          inplace=True)\n",
    "df['Region ID'] = df['Region']\n",
    "df['Region ID'].replace({'Central': 0, 'Yakutia': 1}, inplace=True)\n",
    "df['Prediction ID'] = df['Prediction']\n",
    "df['Prediction ID'].replace({'Central': 0, 'Yakutia': 1}, inplace=True)\n",
    "df.to_excel(f\"{loaded_model.config['checkpoints_path']}/candidates/{model_id}/df.xlsx\")\n",
    "\n",
    "colors_groups = {\n",
    "    'Train': 'chartreuse',\n",
    "    'Validation': 'lightskyblue',\n",
    "    'Test': 'dodgerblue',\n",
    "}\n",
    "\n",
    "metrics_w_avg = [\n",
    "    \"accuracy\",\n",
    "    \"f1_score\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"specificity\",\n",
    "    \"auroc\"\n",
    "]\n",
    "metrics_wo_avg = [\n",
    "    \"cohen_kappa\"\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    index=[f\"{m}_macro\" for m in metrics_w_avg] +\n",
    "          [f\"{m}_weighted\" for m in metrics_w_avg] +\n",
    "          metrics_wo_avg,\n",
    "    columns=list(colors_groups.keys()),\n",
    ")\n",
    "for group in colors_groups.keys():\n",
    "    pred = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction ID'].values)\n",
    "    probs = torch.from_numpy(df.loc[df['Group'] == group, ['Central_probability', 'Yakutia_probability']].values)\n",
    "    real = torch.from_numpy(df.loc[df['Group'] == group, 'Region ID'].values)\n",
    "    for avg_type in ['macro', 'weighted']:\n",
    "        df_metrics.at[f\"accuracy_{avg_type}\", group] = multiclass_accuracy(preds=pred, target=real, num_classes=2, average=avg_type).numpy()\n",
    "        df_metrics.at[f\"f1_score_{avg_type}\", group] = multiclass_f1_score(preds=pred, target=real, num_classes=2, average=avg_type).numpy()\n",
    "        df_metrics.at[f\"precision_{avg_type}\", group] = multiclass_precision(preds=pred, target=real, num_classes=2, average=avg_type).numpy()\n",
    "        df_metrics.at[f\"recall_{avg_type}\", group] = multiclass_recall(preds=pred, target=real, num_classes=2, average=avg_type).numpy()\n",
    "        df_metrics.at[f\"specificity_{avg_type}\", group] = multiclass_specificity(preds=pred, target=real, num_classes=2, average=avg_type).numpy()\n",
    "        df_metrics.at[f\"auroc_{avg_type}\", group] = multiclass_auroc(preds=probs, target=real, num_classes=2, average=avg_type).numpy()\n",
    "    df_metrics.at[\"cohen_kappa\", group] = multiclass_cohen_kappa(preds=pred, target=real, num_classes=2).numpy()\n",
    "df_metrics.to_excel(f\"{loaded_model.config['checkpoints_path']}/candidates/{model_id}/metrics.xlsx\", index_label=\"Metrics\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple TabularModel training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "trainer_config['checkpoints'] = 'valid_loss'\n",
    "trainer_config['load_best'] = True\n",
    "trainer_config['auto_lr_find'] = True\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=f\"{path_configs}/DataConfig.yaml\",\n",
    "    model_config=f\"{path_configs}/models/CategoryEmbeddingModelConfig.yaml\",\n",
    "    optimizer_config=f\"{path_configs}/OptimizerConfig.yaml\",\n",
    "    trainer_config=trainer_config,\n",
    "    verbose=True,\n",
    "    suppress_lightning_logger=False\n",
    ")\n",
    "\n",
    "tabular_model.fit(\n",
    "    train=train_only,\n",
    "    validation=validation_only,\n",
    "    # target_transform=[np.log, np.exp],\n",
    "    # callbacks=[DeviceStatsMonitor()],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Play with trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model.predict(test, progress_bar='rich')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model.evaluate(test, verbose=True, ckpt_path=\"best\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model.config['checkpoints_path']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(tabular_model.trainer.checkpoint_callback.best_model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model.save_model(tabular_model.config['checkpoints_path'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model.save_config(tabular_model.config['checkpoints_path'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tabular_model = TabularModel.load_model(tabular_model.config['checkpoints_path'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
