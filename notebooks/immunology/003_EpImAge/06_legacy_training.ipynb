{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Debugging autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytorch_tabular.utils import load_covertype_dataset\n",
    "from rich.pretty import pprint\n",
    "from sklearn.model_selection import BaseCrossValidator, ParameterGrid, ParameterSampler\n",
    "import torch\n",
    "import pickle\n",
    "import shap\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from glob import glob\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pytorch_tabular.utils import make_mixed_dataset, print_metrics\n",
    "from pytorch_tabular import available_models\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.tabular_model_tuner import TabularModelTuner\n",
    "from torchmetrics.functional.regression import mean_absolute_error, pearson_corrcoef\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "import pandas as pd\n",
    "from pytorch_tabular import model_sweep\n",
    "from src.pt.model_sweep import model_sweep_custom\n",
    "import warnings\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.utils.hash import dict_hash\n",
    "from src.pt.hyper_opt import train_hyper_opt\n",
    "import optuna\n",
    "import pathlib\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*exists and is not empty.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*is smaller than the logging interval Trainer.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prepaer data for legacy training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load immunomarker regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epi_data_type = 'no_harm'\n",
    "imm_data_type = 'imp_source(imm)_method(knn)_params(5)' # 'origin' 'imp_source(imm)_method(knn)_params(5)' 'imp_source(imm)_method(miceforest)_params(2)'\n",
    "\n",
    "selection_method = 'mrmr' # 'f_regression' 'spearman' 'mrmr'\n",
    "n_feats = 100\n",
    "\n",
    "imm = 'CXCL9'\n",
    "\n",
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = 1337\n",
    "tst_split_id = 5\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 2\n",
    "val_random_state = 1337\n",
    "val_fold_id = 5\n",
    "\n",
    "fn_samples = f\"samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats})\"\n",
    "with open(f\"D:/YandexDisk/Work/bbd/immunology/003_EpImAge/{fn_samples}.pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)\n",
    "    \n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")\n",
    "\n",
    "path_data = f\"D:/YandexDisk/Work/bbd/immunology/003_EpImAge/{imm_data_type}/{epi_data_type}/{selection_method}_{n_feats}/{imm}\"\n",
    "pathlib.Path(f\"{path_data}/pytorch_tabular\").mkdir(parents=True, exist_ok=True)\n",
    "path_configs = \"D:/Work/bbs/notebooks/immunology/003_EpImAge/immuno_regression_configs\"\n",
    "data = pd.read_excel(f\"{path_data}/data.xlsx\", index_col=0)\n",
    "feats = pd.read_excel(f\"{path_data}/feats_con.xlsx\", index_col=0).index.values.tolist()\n",
    "\n",
    "split_dict = samples[tst_split_id]\n",
    "\n",
    "test = data.loc[split_dict['test'], feats + [f\"{imm}_log\"]]\n",
    "train = data.loc[split_dict['trains'][val_fold_id], feats + [f\"{imm}_log\"]]\n",
    "validation = data.loc[split_dict['validations'][val_fold_id], feats + [f\"{imm}_log\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load age regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_data_type = 'no_harm'\n",
    "imm_data_type = 'imp_source(imm)_method(knn)_params(5)' # 'origin' 'imp_source(imm)_method(knn)_params(5)' 'imp_source(imm)_method(miceforest)_params(2)'\n",
    "\n",
    "selection_method = 'mrmr' # 'f_regression' 'spearman' 'mrmr'\n",
    "n_feats = 100\n",
    "\n",
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = 1337\n",
    "tst_split_id = 20\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 4\n",
    "val_random_state = 1337\n",
    "val_fold_id = 5\n",
    "\n",
    "path_data = f\"E:/YandexDisk/Work/bbd/immunology/003_EpImAge/{imm_data_type}/{epi_data_type}/{selection_method}_{n_feats}/EpImAge\"\n",
    "\n",
    "\n",
    "data = pd.read_excel(f\"{path_data}/data_filtered.xlsx\", index_col=0)\n",
    "data = data.loc[data['Status'] == 'Control', :]\n",
    "data['Index'] = data.index.values\n",
    "\n",
    "feats = pd.read_excel(f\"{path_data}/feats.xlsx\", index_col=0).index.values.tolist()\n",
    "feats = [f\"{f}_log\" for f in feats]\n",
    "\n",
    "with open(f\"{path_data}/samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats}).pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")\n",
    "\n",
    "split_dict = samples[tst_split_id]\n",
    "\n",
    "test = data.loc[split_dict['test'], feats + [\"Age\"]]\n",
    "train = data.loc[split_dict['trains'][val_fold_id], feats + [\"Age\"]]\n",
    "validation = data.loc[split_dict['validations'][val_fold_id], feats + [\"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add train, validation, test info in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[split_dict['test'], 'Split'] = 'tst'\n",
    "data.loc[split_dict['trains'][val_fold_id], 'Split'] = 'trn'\n",
    "data.loc[split_dict['validations'][val_fold_id], 'Split'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(f\"{path_data}/data_legacy.xlsx\")\n",
    "data.to_pickle(f\"{path_data}/data_legacy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats = pd.DataFrame(index=feats)\n",
    "df_feats.to_excel((f\"{path_data}/feats_con_legacy.xlsx\"), index_label='Feats')\n",
    "df_feats.to_pickle((f\"{path_data}/feats_con_legacy.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect legacy results: immunomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_data_type = 'no_harm'\n",
    "imm_data_type = 'imp_source(imm)_method(knn)_params(5)' # 'origin' 'imp_source(imm)_method(knn)_params(5)' 'imp_source(imm)_method(miceforest)_params(2)'\n",
    "\n",
    "selection_method = 'mrmr' # 'f_regression' 'spearman' 'mrmr'\n",
    "n_feats = 100\n",
    "\n",
    "imm = 'CXCL9'\n",
    "\n",
    "model = 'elastic_net'\n",
    "\n",
    "path_runs = f\"E:/YandexDisk/Work/bbd/immunology/003_EpImAge/{imm_data_type}/{epi_data_type}/{selection_method}_{n_feats}/{imm}/models/{model}_trn_val_tst/multiruns\"\n",
    "\n",
    "files = glob(f\"{path_runs}/*/*/metrics_all_best_*.xlsx\")\n",
    "\n",
    "df_tmp = pd.read_excel(files[0], index_col=\"metric\")\n",
    "head, tail = os.path.split(files[0])\n",
    "cfg = OmegaConf.load(f\"{head}/.hydra/overrides.yaml\")\n",
    "params = []\n",
    "for param_pair in cfg:\n",
    "    param, val = param_pair.split('=')\n",
    "    params.append(param)\n",
    "df_res = pd.DataFrame(index=files)\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    head, tail = os.path.split(file)\n",
    "    \n",
    "    df_res.at[file, 'index'] = head.replace(path_runs, '')\n",
    "    # Metrics\n",
    "    df_metrics = pd.read_excel(file, index_col=\"metric\")\n",
    "    for metric in df_metrics.index.values:\n",
    "        df_res.at[file, metric + \"_val\"] = df_metrics.at[metric, \"val\"]\n",
    "        df_res.at[file, metric + \"_trn\"] = df_metrics.at[metric, \"trn\"]\n",
    "        df_res.at[file, metric + \"_tst\"] = df_metrics.at[metric, \"tst\"]\n",
    "        df_res.at[file, metric + \"_trn_val\"] = df_metrics.at[metric, \"trn_val\"]\n",
    "        df_res.at[file, metric + \"_val_tst\"] = df_metrics.at[metric, \"val_tst\"]\n",
    "        df_res.at[file, metric + \"_trn_val_tst\"] = df_metrics.at[metric, \"trn_val_tst\"]\n",
    "\n",
    "    # Params\n",
    "    cfg = OmegaConf.load(f\"{head}/.hydra/overrides.yaml\")\n",
    "    for param_pair in cfg:\n",
    "        param, val = param_pair.split('=')\n",
    "        df_res.at[file, param] = val\n",
    "\n",
    "df_res.set_index('index', inplace=True)\n",
    "df_res[\"train_more_val\"] = False\n",
    "df_res[\"selected\"] = False\n",
    "df_res.loc[df_res[\"mean_absolute_error_trn\"] > df_res[\"mean_absolute_error_val\"], \"train_more_val\"] = True\n",
    "\n",
    "first_columns = {\n",
    "    'selected': 'selected',\n",
    "    'train_more_val': 'train_more_val',\n",
    "    'mean_absolute_error_trn': 'MAE trn',\n",
    "    'mean_absolute_error_val': 'MAE val',\n",
    "    'mean_absolute_error_tst': 'MAE tst',\n",
    "    'mean_absolute_error_val_tst': 'MAE val_tst',\n",
    "    'mean_absolute_error_trn_val_tst': 'MAE trn_val_tst',\n",
    "    'pearson_corr_coef_trn': 'Pcorr trn',\n",
    "    'pearson_corr_coef_val': 'Pcorr val',\n",
    "    'pearson_corr_coef_tst': 'Pcorr tst',\n",
    "    'pearson_corr_coef_val_tst': 'Pcorr val_tst',\n",
    "    'pearson_corr_coef_trn_val_tst': 'Pcorr trn_val_tst',\n",
    "    'mean_absolute_error_cv_mean_trn': 'MAE cv_mean_trn',\n",
    "    'mean_absolute_error_cv_std_trn': 'MAE cv_std_trn',\n",
    "    'mean_absolute_error_cv_mean_val': 'MAE cv_mean_val',\n",
    "    'mean_absolute_error_cv_std_val': 'MAE cv_std_val',\n",
    "    'pearson_corr_coef_cv_mean_trn': 'Pcorr cv_mean_trn',\n",
    "    'pearson_corr_coef_cv_std_trn': 'Pcorr cv_std_trn',\n",
    "    'pearson_corr_coef_cv_mean_val': 'Pcorr cv_mean_val',\n",
    "    'pearson_corr_coef_cv_std_val': 'Pcorr cv_std_val',\n",
    "}\n",
    "df_res = df_res[list(first_columns.keys()) + [col for col in df_res.columns if col not in first_columns]]\n",
    "df_res.rename(columns=first_columns, inplace=True)\n",
    "df_res.to_excel(f\"{path_runs}/summary.xlsx\", index=True, index_label=\"file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect legacy results: age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_data_type = 'no_harm'\n",
    "imm_data_type = 'imp_source(imm)_method(knn)_params(5)' # 'origin' 'imp_source(imm)_method(knn)_params(5)' 'imp_source(imm)_method(miceforest)_params(2)'\n",
    "\n",
    "selection_method = 'mrmr' # 'f_regression' 'spearman' 'mrmr'\n",
    "n_feats = 100\n",
    "\n",
    "\n",
    "model = 'elastic_net'\n",
    "\n",
    "path_runs = f\"E:/YandexDisk/Work/bbd/immunology/003_EpImAge/{imm_data_type}/{epi_data_type}/{selection_method}_{n_feats}/EpImAge/models/{model}_trn_val_tst/multiruns\"\n",
    "\n",
    "files = glob(f\"{path_runs}/*/*/metrics_all_best_*.xlsx\")\n",
    "\n",
    "df_tmp = pd.read_excel(files[0], index_col=\"metric\")\n",
    "head, tail = os.path.split(files[0])\n",
    "cfg = OmegaConf.load(f\"{head}/.hydra/overrides.yaml\")\n",
    "params = []\n",
    "for param_pair in cfg:\n",
    "    param, val = param_pair.split('=')\n",
    "    params.append(param)\n",
    "df_res = pd.DataFrame(index=files)\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    head, tail = os.path.split(file)\n",
    "    \n",
    "    df_res.at[file, 'index'] = head.replace(path_runs, '')\n",
    "    # Metrics\n",
    "    df_metrics = pd.read_excel(file, index_col=\"metric\")\n",
    "    for metric in df_metrics.index.values:\n",
    "        df_res.at[file, metric + \"_val\"] = df_metrics.at[metric, \"val\"]\n",
    "        df_res.at[file, metric + \"_trn\"] = df_metrics.at[metric, \"trn\"]\n",
    "        df_res.at[file, metric + \"_tst\"] = df_metrics.at[metric, \"tst\"]\n",
    "        df_res.at[file, metric + \"_trn_val\"] = df_metrics.at[metric, \"trn_val\"]\n",
    "        df_res.at[file, metric + \"_val_tst\"] = df_metrics.at[metric, \"val_tst\"]\n",
    "        df_res.at[file, metric + \"_trn_val_tst\"] = df_metrics.at[metric, \"trn_val_tst\"]\n",
    "\n",
    "    # Params\n",
    "    cfg = OmegaConf.load(f\"{head}/.hydra/overrides.yaml\")\n",
    "    for param_pair in cfg:\n",
    "        param, val = param_pair.split('=')\n",
    "        df_res.at[file, param] = val\n",
    "\n",
    "df_res.set_index('index', inplace=True)\n",
    "df_res[\"train_more_val\"] = False\n",
    "df_res[\"selected\"] = False\n",
    "df_res.loc[df_res[\"mean_absolute_error_trn\"] > df_res[\"mean_absolute_error_val\"], \"train_more_val\"] = True\n",
    "\n",
    "first_columns = {\n",
    "    'selected': 'selected',\n",
    "    'train_more_val': 'train_more_val',\n",
    "    'mean_absolute_error_trn': 'MAE trn',\n",
    "    'mean_absolute_error_val': 'MAE val',\n",
    "    'mean_absolute_error_tst': 'MAE tst',\n",
    "    'mean_absolute_error_val_tst': 'MAE val_tst',\n",
    "    'mean_absolute_error_trn_val_tst': 'MAE trn_val_tst',\n",
    "    'pearson_corr_coef_trn': 'Pcorr trn',\n",
    "    'pearson_corr_coef_val': 'Pcorr val',\n",
    "    'pearson_corr_coef_tst': 'Pcorr tst',\n",
    "    'pearson_corr_coef_val_tst': 'Pcorr val_tst',\n",
    "    'pearson_corr_coef_trn_val_tst': 'Pcorr trn_val_tst',\n",
    "    'mean_absolute_error_cv_mean_trn': 'MAE cv_mean_trn',\n",
    "    'mean_absolute_error_cv_std_trn': 'MAE cv_std_trn',\n",
    "    'mean_absolute_error_cv_mean_val': 'MAE cv_mean_val',\n",
    "    'mean_absolute_error_cv_std_val': 'MAE cv_std_val',\n",
    "    'pearson_corr_coef_cv_mean_trn': 'Pcorr cv_mean_trn',\n",
    "    'pearson_corr_coef_cv_std_trn': 'Pcorr cv_std_trn',\n",
    "    'pearson_corr_coef_cv_mean_val': 'Pcorr cv_mean_val',\n",
    "    'pearson_corr_coef_cv_std_val': 'Pcorr cv_std_val',\n",
    "}\n",
    "df_res = df_res[list(first_columns.keys()) + [col for col in df_res.columns if col not in first_columns]]\n",
    "df_res.rename(columns=first_columns, inplace=True)\n",
    "df_res.to_excel(f\"{path_runs}/summary.xlsx\", index=True, index_label=\"file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
