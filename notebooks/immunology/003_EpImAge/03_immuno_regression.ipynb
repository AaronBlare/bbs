{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Debugging autoreload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pytorch_tabular.utils import load_covertype_dataset\n",
    "from rich.pretty import pprint\n",
    "import torch\n",
    "import shap\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from glob import glob\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pytorch_tabular.utils import make_mixed_dataset, print_metrics\n",
    "from pytorch_tabular import available_models\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.tabular_model_tuner import TabularModelTuner\n",
    "from torchmetrics.functional.regression import mean_absolute_error, pearson_corrcoef\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "import pandas as pd\n",
    "from pytorch_tabular import model_sweep\n",
    "from src.pt.model_sweep import model_sweep_custom\n",
    "import warnings\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.utils.hash import dict_hash\n",
    "import pathlib\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "epi_dataa_type = 'no_harm'\n",
    "\n",
    "selection_method = 'f_regression' # 'f_regression' 'spearman'\n",
    "n_feats = 100\n",
    "\n",
    "imm = 'CXCL9'\n",
    "\n",
    "test_split_id = 2\n",
    "val_n_splits = 4\n",
    "val_random_state = 1337\n",
    "val_fold_id = 0\n",
    "\n",
    "path_data = f\"D:/YandexDisk/Work/bbd/immunology/003_EpImAge/{epi_dataa_type}/{selection_method}_{n_feats}/{imm}\"\n",
    "pathlib.Path(f\"{path_data}/pytorch_tabular/trials\").mkdir(parents=True, exist_ok=True)\n",
    "path_configs = \"D:/Work/bbs/notebooks/immunology/003_EpImAge/immuno_regression_configs\"\n",
    "data = pd.read_excel(f\"{path_data}/data.xlsx\", index_col=0)\n",
    "feats = pd.read_excel(f\"{path_data}/feats_con.xlsx\", index_col=0).index.values.tolist()\n",
    "\n",
    "for fold_id in range(val_n_splits):\n",
    "    data[f\"Fold_{fold_id}\"] = data[f\"Split_{test_split_id}\"]\n",
    "\n",
    "stratify_cat_parts = {\n",
    "    'ctrl_central': data.index[(data['Status'] == 'Control') & (data['Region'] == 'Central') & (data[f\"Split_{test_split_id}\"] == 'trn_val')].values,\n",
    "    'ctrl_yakutia': data.index[(data['Status'] == 'Control') & (data['Region'] == 'Yakutia') & (data[f\"Split_{test_split_id}\"] == 'trn_val')].values,\n",
    "    'esrd': data.index[(data['Status'] == 'ESRD') & (data[f\"Split_{test_split_id}\"] == 'trn_val')].values,\n",
    "}\n",
    "for part, ids in stratify_cat_parts.items():\n",
    "    print(f\"{part}: {len(ids)}\")\n",
    "    con = data.loc[ids, 'Age'].values\n",
    "    ptp = np.ptp(con)\n",
    "    num_bins = 5\n",
    "    bins = np.linspace(np.min(con) - 0.1 * ptp, np.max(con) + 0.1 * ptp, num_bins + 1)\n",
    "    binned = np.digitize(con, bins) - 1\n",
    "    unique, counts = np.unique(binned, return_counts=True)\n",
    "    occ = dict(zip(unique, counts))\n",
    "    k_fold = RepeatedStratifiedKFold(\n",
    "        n_splits=val_n_splits,\n",
    "        n_repeats=1,\n",
    "        random_state=val_random_state\n",
    "    )\n",
    "    splits = k_fold.split(X=ids, y=binned, groups=binned)\n",
    "    \n",
    "    for fold_id, (ids_trn, ids_val) in enumerate(splits):\n",
    "        data.loc[ids[ids_trn], f\"Fold_{fold_id}\"] = \"trn\"\n",
    "        data.loc[ids[ids_val], f\"Fold_{fold_id}\"] = \"val\"\n",
    "\n",
    "test = data.loc[data[f\"Split_{test_split_id}\"] == \"tst\", feats + [f\"{imm}_log\"]]\n",
    "train_validation = data.loc[data[f\"Split_{test_split_id}\"] == \"trn_val\", feats + [f\"{imm}_log\"] + [f\"Fold_{i}\" for i in range(val_n_splits)]]\n",
    "train_only = data.loc[data[f\"Fold_{val_fold_id}\"] == \"trn\", feats + [f\"{imm}_log\"]]\n",
    "validation_only = data.loc[data[f\"Fold_{val_fold_id}\"] == \"val\", feats + [f\"{imm}_log\"]]\n",
    "cv_indexes = [\n",
    "    (\n",
    "        np.where(train_validation.index.isin(train_validation.index[train_validation[f\"Fold_{i}\"] == 'trn']))[0],\n",
    "        np.where(train_validation.index.isin(train_validation.index[train_validation[f\"Fold_{i}\"] == 'val']))[0],\n",
    "    )\n",
    "    for i in range(val_n_splits)\n",
    "]\n",
    "\n",
    "trains = {}\n",
    "validations = {}\n",
    "for fold_id in range(val_n_splits):\n",
    "    trains[fold_id] = data.loc[data[f\"Fold_{fold_id}\"] == \"trn\", feats + [f\"{imm}_log\"]]\n",
    "    validations[fold_id] = data.loc[data[f\"Fold_{fold_id}\"] == \"val\", feats + [f\"{imm}_log\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load non-model configs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "data_config['target'] = [f\"{imm}_log\"]\n",
    "data_config['continuous_cols'] = feats\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "trainer_config['checkpoints_path'] = f\"{path_data}/pytorch_tabular\"\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models Search Spaces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CategoryEmbeddingModel Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__layers\": [\"256-128-64\", \"512-256-128\", \"512-256-256-128\", \"32-16\", \"32-32-16\", \"64-32-16\", \"16-8\", \"32-16-8\", \"128-64\", \"128-128\", \"16-16\", \"128-128-64\"],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/CategoryEmbeddingModelConfig.yaml\", CategoryEmbeddingModelConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GANDALF Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__gflu_stages\": [3, 6, 10, 15, 20, 25, 30, 35],\n",
    "    \"model_config__gflu_dropout\": [0.0, 0.1],\n",
    "    \"model_config__gflu_feature_init_sparsity\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/GANDALFConfig.yaml\", GANDALFConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TabNetModel Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__n_d\": [4, 8, 16, 32],\n",
    "    \"model_config__n_a\": [4, 8, 16, 32],\n",
    "    \"model_config__n_steps\": [3, 5, 7],\n",
    "    \"model_config__gamma\": [1.2, 1.3, 1.4, 1.5],\n",
    "    \"model_config__n_independent\": [1, 2, 3, 4],\n",
    "    \"model_config__n_shared\": [1, 2, 3, 4],\n",
    "    \"model_config__mask_type\": [\"sparsemax\", \"entmax\"],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/TabNetModelConfig.yaml\", TabNetModelConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DANet Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__n_layers\": [4, 8, 16, 20, 32],\n",
    "    \"model_config__abstlay_dim_1\": [16, 32, 64],\n",
    "    \"model_config__k\": [4, 5, 6],\n",
    "    \"model_config__dropout_rate\": [0.1],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/DANetConfig.yaml\", DANetConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FTTransformer Search Space"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"model_config__num_heads\": [4, 8, 16],\n",
    "    \"model_config__num_attn_blocks\": [4, 6, 8, 10],\n",
    "    \"model_config__attn_dropout\": [0.0, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "    \"model_config__add_norm_dropout\": [0.1],\n",
    "    \"model_config__ff_dropout\": [0.1],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1337],\n",
    "}\n",
    "model_config = read_parse_config(f\"{path_configs}/models/FTTransformerConfig.yaml\", FTTransformerConfig)\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grid Search and Random Search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "\n",
    "strategy = 'random_search' # 'grid_search'\n",
    "seed = 1337\n",
    "n_random_trials = 100\n",
    "is_cross_validation = True\n",
    "\n",
    "if grid_size < n_random_trials and strategy == 'random_search':\n",
    "    strategy = 'grid_search'\n",
    "\n",
    "trainer_config['checkpoints'] = None\n",
    "trainer_config['load_best'] = False\n",
    "trainer_config['auto_lr_find'] = True\n",
    "\n",
    "tuner = TabularModelTuner(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    suppress_lightning_logger=True,\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    if is_cross_validation:\n",
    "        result = tuner.tune(\n",
    "            train=train_validation,\n",
    "            validation=None,\n",
    "            search_space=search_space,\n",
    "            metric=\"mean_absolute_error\",\n",
    "            mode=\"min\",\n",
    "            strategy=strategy,\n",
    "            n_trials=n_random_trials,\n",
    "            cv=cv_indexes,\n",
    "            return_best_model=True,\n",
    "            verbose=False,\n",
    "            progress_bar=True,\n",
    "            random_state=seed,\n",
    "        )\n",
    "    else:\n",
    "        result = tuner.tune(\n",
    "            train=train_only,\n",
    "            validation=validation_only,\n",
    "            search_space=search_space,\n",
    "            metric=\"mean_absolute_error\",\n",
    "            mode=\"min\",\n",
    "            strategy=strategy,\n",
    "            n_trials=n_random_trials,\n",
    "            cv=None,\n",
    "            return_best_model=True,\n",
    "            verbose=False,\n",
    "            progress_bar=False,\n",
    "            random_state=seed,\n",
    "        )\n",
    "result.trials_df.to_excel(f\"{trainer_config['checkpoints_path']}/trials/{model_config['_model_name']}_{strategy}_{seed}_{optimizer_config['lr_scheduler']}.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Sweep Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate models' configs from trials files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_top_trials = 5\n",
    "\n",
    "target_models_types = [\n",
    "    # 'CategoryEmbeddingModel',\n",
    "    'GANDALF',\n",
    "    # 'TabNetModel',\n",
    "    # 'FTTransformer',\n",
    "    'DANet'\n",
    "]\n",
    "\n",
    "common_params = {\n",
    "    \"task\": \"regression\",\n",
    "}\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    activation='ReLU',\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=False,\n",
    "    initialization=\"kaiming\"\n",
    ").__dict__\n",
    "\n",
    "model_list = []\n",
    "for model_type in target_models_types:\n",
    "    trials_files = glob(f\"{trainer_config['checkpoints_path']}/trials/{model_type}*.xlsx\")\n",
    "    for trials_file in trials_files:\n",
    "        df_trials = pd.read_excel(trials_file, index_col=0)\n",
    "        df_trials.sort_values(['mean_absolute_error'], ascending=[True], inplace=True)\n",
    "        df_trials = df_trials.head(n_top_trials)\n",
    "        for _, row in df_trials.iterrows():\n",
    "            head_config_tmp = copy.deepcopy(head_config)\n",
    "            head_config_tmp['dropout'] = float(row['model_config.head_config__dropout'])\n",
    "            if model_type == 'CategoryEmbeddingModel':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", CategoryEmbeddingModelConfig)\n",
    "                model_config['layers'] = row['model_config__layers']\n",
    "                model_config['learning_rate'] = row['model_config__learning_rate']\n",
    "                model_config['seed'] = row['model_config__seed']\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(CategoryEmbeddingModelConfig(**model_config))\n",
    "            elif model_type == 'GANDALF':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", GANDALFConfig)\n",
    "                model_config['gflu_stages'] = int(row['model_config__gflu_stages'])\n",
    "                model_config['gflu_feature_init_sparsity'] = float(row['model_config__gflu_feature_init_sparsity'])\n",
    "                model_config['gflu_dropout'] = float(row['model_config__gflu_dropout'])\n",
    "                model_config['learning_rate'] = float(row['model_config__learning_rate'])\n",
    "                model_config['seed'] = int(row['model_config__seed'])\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(GANDALFConfig(**model_config))\n",
    "            elif model_type == 'TabNetModel':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", TabNetModelConfig)\n",
    "                model_config['n_steps'] = row['model_config__n_steps']\n",
    "                model_config['n_shared'] = row['model_config__n_shared']\n",
    "                model_config['n_independent'] = row['model_config__n_independent']\n",
    "                model_config['n_d'] = row['model_config__n_d']\n",
    "                model_config['n_a'] = row['model_config__n_a']\n",
    "                model_config['mask_type'] = row['model_config__mask_type']\n",
    "                model_config['gamma'] = row['model_config__gamma']\n",
    "                model_config['learning_rate'] = row['model_config__learning_rate']\n",
    "                model_config['seed'] = row['model_config__seed']\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(TabNetModelConfig(**model_config))\n",
    "            elif model_type == 'FTTransformer':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", FTTransformerConfig)\n",
    "                model_config['num_heads'] = int(row['model_config__num_heads'])\n",
    "                model_config['num_attn_blocks'] = int(row['model_config__num_attn_blocks'])\n",
    "                model_config['attn_dropout'] = float(row['model_config__attn_dropout'])\n",
    "                model_config['add_norm_dropout'] = float(row['model_config__add_norm_dropout'])\n",
    "                model_config['ff_dropout'] = float(row['model_config__ff_dropout'])\n",
    "                model_config['learning_rate'] = float(row['model_config__learning_rate'])\n",
    "                model_config['seed'] = int(row['model_config__seed'])\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(FTTransformerConfig(**model_config))\n",
    "            elif model_type == 'DANet':\n",
    "                model_config = read_parse_config(f\"{path_configs}/models/{model_type}Config.yaml\", DANetConfig)\n",
    "                model_config['n_layers'] = int(row['model_config__n_layers'])\n",
    "                model_config['abstlay_dim_1'] = int(row['model_config__abstlay_dim_1'])\n",
    "                model_config['k'] = int(row['model_config__k'])\n",
    "                model_config['dropout_rate'] = float(row['model_config__dropout_rate'])\n",
    "                model_config['learning_rate'] = float(row['model_config__learning_rate'])\n",
    "                model_config['seed'] = int(row['model_config__seed'])\n",
    "                model_config['head_config'] = head_config_tmp\n",
    "                model_list.append(DANetConfig(**model_config))\n",
    "print(len(model_list))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform model sweep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "for fold_id in range(val_n_splits):\n",
    "    \n",
    "    for seed in [1337, 55763, 40279, 87571, 234461]:\n",
    "    \n",
    "        trainer_config['seed'] = seed\n",
    "        trainer_config['checkpoints'] = 'valid_loss'\n",
    "        trainer_config['load_best'] = True\n",
    "        trainer_config['auto_lr_find'] = True\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            sweep_df, best_model = model_sweep_custom(\n",
    "                task=\"regression\",\n",
    "                # train=train_validation,\n",
    "                train=trains[fold_id],\n",
    "                # validation=None,\n",
    "                validation=validations[fold_id],\n",
    "                test=test,\n",
    "                data_config=data_config,\n",
    "                optimizer_config=optimizer_config,\n",
    "                trainer_config=trainer_config,\n",
    "                model_list=model_list,\n",
    "                common_model_args=common_params,\n",
    "                metrics=[\"mean_absolute_error\", \"pearson_corrcoef\"],\n",
    "                metrics_params=[{}, {}],\n",
    "                metrics_prob_input=[False, False],\n",
    "                rank_metric=(\"mean_absolute_error\", \"lower_is_better\"),\n",
    "                return_best_model=True,\n",
    "                seed=seed,\n",
    "                progress_bar=False,\n",
    "                verbose=False,\n",
    "                suppress_lightning_logger=True,\n",
    "            )\n",
    "        fn_suffix = f\"{fold_id}_{seed}_{best_model.config['lr_scheduler']}_{best_model.config['continuous_feature_transform']}\"\n",
    "        sweep_df.style.background_gradient(\n",
    "            subset=[\n",
    "                \"train_loss\",\n",
    "                \"validation_loss\",\n",
    "                \"test_loss\",\n",
    "                \"time_taken\",\n",
    "                \"time_taken_per_epoch\"\n",
    "            ], cmap=\"RdYlGn_r\"\n",
    "        ).to_excel(f\"{trainer_config['checkpoints_path']}/sweep_{fn_suffix}.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save best models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "\n",
    "selected_models_sweeps = [\n",
    "    {'fold_id': 0, 'seed': 1337, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [11]},\n",
    "    {'fold_id': 0, 'seed': 40279, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [0]},\n",
    "    {'fold_id': 0, 'seed': 55763, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [11, 0]},\n",
    "    {'fold_id': 0, 'seed': 87571, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [13]},\n",
    "    {'fold_id': 0, 'seed': 234461, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [11]},\n",
    "    {'fold_id': 1, 'seed': 1337, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [1]},\n",
    "    {'fold_id': 1, 'seed': 40279, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [10]},\n",
    "    {'fold_id': 1, 'seed': 55763, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [13]},\n",
    "    {'fold_id': 1, 'seed': 234461, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [4]},\n",
    "    {'fold_id': 3, 'seed': 87571, 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'continuous_feature_transform': 'yeo-johnson', 'ids': [7]},\n",
    "]\n",
    "\n",
    "explain_method = \"GradientShap\"\n",
    "explain_baselines = \"b|1000\"\n",
    "explain_n_feats_to_plot = 25\n",
    "\n",
    "for sweep in selected_models_sweeps:\n",
    "    \n",
    "    data_config['continuous_feature_transform'] = sweep['continuous_feature_transform']\n",
    "    \n",
    "    sweep_suffix = f\"{sweep['fold_id']}_{sweep['seed']}_{optimizer_config['lr_scheduler']}_{sweep['continuous_feature_transform']}\"\n",
    "    sweep_df = pd.read_excel(f\"{trainer_config['checkpoints_path']}/sweep_{sweep_suffix}.xlsx\", index_col=0)\n",
    "    \n",
    "    path_models = f\"{trainer_config['checkpoints_path']}/candidates/{sweep_suffix}\"\n",
    "    pathlib.Path(path_models).mkdir(parents=True, exist_ok=True)\n",
    "    sweep_df.style.background_gradient(\n",
    "            subset=[\n",
    "                \"train_loss\",\n",
    "                \"validation_loss\",\n",
    "                \"test_loss\",\n",
    "                \"time_taken\",\n",
    "                \"time_taken_per_epoch\"\n",
    "            ], cmap=\"RdYlGn_r\"\n",
    "        ).to_excel(f\"{path_models}/sweep.xlsx\")\n",
    "    \n",
    "    models_ids = sweep['ids']\n",
    "\n",
    "    for model_id in models_ids:\n",
    "    \n",
    "        tabular_model = TabularModel(\n",
    "            data_config=data_config,\n",
    "            model_config=ast.literal_eval(sweep_df.at[model_id, 'params']),\n",
    "            optimizer_config=optimizer_config,\n",
    "            trainer_config=trainer_config,\n",
    "            verbose=True,\n",
    "            suppress_lightning_logger=True\n",
    "        )\n",
    "        datamodule = tabular_model.prepare_dataloader(\n",
    "            train=trains[sweep['fold_id']],\n",
    "            validation=validations[sweep['fold_id']],\n",
    "            seed=sweep['seed'],\n",
    "        )\n",
    "        model = tabular_model.prepare_model(\n",
    "            datamodule\n",
    "        )\n",
    "        tabular_model._prepare_for_training(\n",
    "            model,\n",
    "            datamodule\n",
    "        )\n",
    "        tabular_model.load_weights(sweep_df.at[model_id, 'checkpoint'])\n",
    "        tabular_model.evaluate(test, verbose=False)\n",
    "        tabular_model.save_model(f\"{path_models}/{model_id}\")\n",
    "        \n",
    "        loaded_model = TabularModel.load_model(f\"{path_models}/{model_id}\")\n",
    "        \n",
    "        df = data.loc[:, data_config['target']]\n",
    "        df.loc[trains[sweep['fold_id']].index, 'Group'] = 'Train'\n",
    "        df.loc[validations[sweep['fold_id']].index, 'Group'] = 'Validation'\n",
    "        df.loc[test.index, 'Group'] = 'Test'\n",
    "        df['Prediction'] = loaded_model.predict(data)\n",
    "        df['Error'] = df['Prediction'] - df[data_config['target'][0]]\n",
    "        df.to_csv(f\"{path_models}/{model_id}/df.xlsx\")\n",
    "        \n",
    "        colors_groups = {\n",
    "            'Train': 'chartreuse',\n",
    "            'Validation': 'dodgerblue',\n",
    "            'Test': 'crimson',\n",
    "        }\n",
    "        \n",
    "        df_metrics = pd.DataFrame(\n",
    "            index=list(colors_groups.keys()),\n",
    "            columns=['mean_absolute_error', 'pearson_corrcoef', 'bias']\n",
    "        )\n",
    "        for group in colors_groups.keys():\n",
    "            pred = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction'].values)\n",
    "            real = torch.from_numpy(df.loc[df['Group'] == group, data_config['target'][0]].values)\n",
    "            df_metrics.at[group, 'mean_absolute_error'] = mean_absolute_error(pred, real).numpy()\n",
    "            df_metrics.at[group, 'pearson_corrcoef'] = pearson_corrcoef(pred, real).numpy()\n",
    "            df_metrics.at[group, 'bias'] = np.mean(df.loc[df['Group'] == group, 'Error'].values)\n",
    "        df_metrics.to_excel(f\"{path_models}/{model_id}/metrics.xlsx\", index_label=\"Metrics\")\n",
    "        \n",
    "        sns.set_theme(style='whitegrid')\n",
    "        xy_min = df[[data_config['target'][0], 'Prediction']].min().min()\n",
    "        xy_max = df[[data_config['target'][0], 'Prediction']].max().max()\n",
    "        xy_ptp = xy_max - xy_min\n",
    "        fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "        scatter = sns.scatterplot(\n",
    "            data=df,\n",
    "            x=data_config['target'][0],\n",
    "            y=\"Prediction\",\n",
    "            hue=\"Group\",\n",
    "            palette=colors_groups,\n",
    "            linewidth=0.2,\n",
    "            alpha=0.75,\n",
    "            edgecolor=\"k\",\n",
    "            s=20,\n",
    "            hue_order=list(colors_groups.keys()),\n",
    "            ax=ax\n",
    "        )\n",
    "        bisect = sns.lineplot(\n",
    "            x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "            y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "            linestyle='--',\n",
    "            color='black',\n",
    "            linewidth=1.0,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"{sweep_df.at[model_id, 'model']} ({sweep_df.at[model_id, '# Params']} params, {sweep_df.at[model_id, 'epochs']} epochs)\")\n",
    "        ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "        ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        fig.savefig(f\"{path_models}/{model_id}/scatter.png\", bbox_inches='tight', dpi=200)\n",
    "        fig.savefig(f\"{path_models}/{model_id}/scatter.pdf\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        df_fig = df.loc[:, ['Error', 'Group']]\n",
    "        groups_rename = {\n",
    "            group: f\"{group}\" + \"\\n\" +\n",
    "                   fr\"MAE: {df_metrics.at[group, 'mean_absolute_error']:0.2f}\" + \"\\n\"\n",
    "                   fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef']:0.2f}\" + \"\\n\" +\n",
    "                   fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias']:0.2f}\" \n",
    "            for group in colors_groups\n",
    "        }\n",
    "        colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "        df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "        sns.set_theme(style='whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        violin = sns.violinplot(\n",
    "            data=df_fig,\n",
    "            x='Group',\n",
    "            y='Error',\n",
    "            palette=colors_groups_violin,\n",
    "            scale='width',\n",
    "            order=list(colors_groups_violin.keys()),\n",
    "            saturation=0.75,\n",
    "            legend=False,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_xlabel('')\n",
    "        fig.savefig(f\"{path_models}/{model_id}/violin.png\", bbox_inches='tight', dpi=200)\n",
    "        fig.savefig(f\"{path_models}/{model_id}/violin.pdf\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        try:\n",
    "            explanation = loaded_model.explain(data, method=explain_method, baselines=explain_baselines)\n",
    "            explanation.index = data.index\n",
    "            explanation.to_excel(f\"{path_models}/{model_id}/explanation.xlsx\")\n",
    "            \n",
    "            sns.set_theme(style='whitegrid')\n",
    "            fig = shap.summary_plot(\n",
    "                shap_values=explanation.loc[:, feats].values,\n",
    "                features=data.loc[:, feats].values,\n",
    "                feature_names=feats,\n",
    "                max_display=explain_n_feats_to_plot,\n",
    "                plot_type=\"violin\",\n",
    "                show=False,\n",
    "            )\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_beeswarm.png\", bbox_inches='tight', dpi=200)\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_beeswarm.pdf\", bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            sns.set_theme(style='whitegrid')\n",
    "            fig = shap.summary_plot(\n",
    "                shap_values=explanation.loc[:, feats].values,\n",
    "                features=data.loc[:, feats].values,\n",
    "                feature_names=feats,\n",
    "                max_display=explain_n_feats_to_plot,\n",
    "                plot_type=\"bar\",\n",
    "                show=False,\n",
    "            )\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_bar.png\", bbox_inches='tight', dpi=200)\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_bar.pdf\", bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "        \n",
    "        except NotImplementedError:\n",
    "            pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
